{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is made for learning purposes and is inspired from work at https://www.kaggle.com/code/priyankdl/word2vec-skipgraom-cbow"
      ],
      "metadata": {
        "id": "tc_DRCLZUd0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ7nwKZopwFV",
        "outputId": "584d3c79-27c1-4364-9f99-9276ae8d6ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2\n",
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dependencies"
      ],
      "metadata": {
        "id": "AE6ZhTZKrefL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the model\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "\n",
        "#For the dataset part\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext import datasets\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4NXWnRpvrc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data part"
      ],
      "metadata": {
        "id": "xClVGa3jr7bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device=torch.device(type='cuda',index=0)\n",
        "else:\n",
        "  device=torch.device(type='cpu',index=0)"
      ],
      "metadata": {
        "id": "FeQ2PYi90aTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=datasets.IMDB(split=\"train\")\n",
        "test_data=datasets.IMDB(split=\"test\")\n",
        "\n",
        "train_reviews=[]\n",
        "test_reviews=[]\n",
        "for label,review in train_data:\n",
        "  train_reviews.append(review)\n",
        "  if (len(train_reviews)>1000):\n",
        "    break\n",
        "\n",
        "for label,review in test_data:\n",
        "  test_reviews.append(review)\n",
        "  if (len(test_reviews)>1000):\n",
        "    break\n",
        "\n",
        "#now we have got the datasets\n",
        "\n",
        "tokenizer=get_tokenizer(\"basic_english\",\"en\")\n",
        "vocab=build_vocab_from_iterator(\n",
        "    map(tokenizer,train_reviews),\n",
        "    specials=[\"<unk>\"],\n",
        "    special_first=False,\n",
        "    min_freq=20\n",
        ")\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "def colate_function(batch,text_pipeline):\n",
        "  input=[]\n",
        "  ground_truth=[]\n",
        "  for review in batch:\n",
        "    indices=vocab.lookup_indices(tokenizer(review))\n",
        "    if (len(indices)<9):\n",
        "      continue\n",
        "    else:\n",
        "      for idx in range(len(indices)-8):\n",
        "        context_window=indices[idx:idx+9]\n",
        "        for i in range(8):\n",
        "          input.append(context_window[4])\n",
        "        for i in range(9):\n",
        "          if (i==4):\n",
        "            continue\n",
        "          else:\n",
        "            ground_truth.append(context_window[i])\n",
        "\n",
        "  input=torch.tensor(input,dtype=torch.long)\n",
        "  ground_truth=torch.tensor(ground_truth,dtype=torch.long)\n",
        "\n",
        "  return input,ground_truth\n",
        "\n",
        "def text_pipeline(review):\n",
        "  return vocab.lookup_indices(tokenizer(review))\n",
        "\n",
        "\n",
        "train_dataloader=DataLoader(\n",
        "    train_reviews,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(colate_function,text_pipeline=text_pipeline)\n",
        ")\n",
        "\n",
        "test_dataloader=DataLoader(\n",
        "    test_reviews,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(colate_function,text_pipeline=text_pipeline)\n",
        ")"
      ],
      "metadata": {
        "id": "8Mm7_1x3r84d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.ebd1=nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=300,\n",
        "        max_norm=1\n",
        "    )\n",
        "\n",
        "    self.linear1=nn.Linear(\n",
        "        in_features=300,\n",
        "        out_features=vocab_size\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.ebd1(x)\n",
        "    x=self.linear1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model=SkipGram(vocab.__len__()).to(device)\n",
        "optimiser=optim.Adam(model.parameters(),lr=0.001)\n",
        "loss_function=nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_one_epoch(model,dataloader,optimiser,loss_function):\n",
        "  model.train()\n",
        "  running_loss=[]\n",
        "\n",
        "  for i,batch_data in enumerate(dataloader):\n",
        "    input=batch_data[0].to(device)\n",
        "    target=batch_data[1].to(device)\n",
        "    output=model(input)\n",
        "    loss=loss_function(output,target)\n",
        "    running_loss.append(loss.item())\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "  print(\"The loss at the end of this epoch is \",np.mean(running_loss))\n",
        "\n",
        "def eval_one_epoch(model,dataloader,loss_function):\n",
        "  model.eval()\n",
        "  running_loss=[]\n",
        "\n",
        "  for i,batch_data in enumerate(dataloader):\n",
        "    input=batch_data[0].to(device)\n",
        "    target=batch_data[1].to(device)\n",
        "    output=model(input)\n",
        "    loss=loss_function(output,target)\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "  print(\"The loss at the end of this epoch is \",np.mean(running_loss))\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  print(\"EPOCH number:\",i+1)\n",
        "  train_one_epoch(model,train_dataloader,optimiser,loss_function)\n",
        "  eval_one_epoch(model,test_dataloader,loss_function)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TpJo1PnwUfO",
        "outputId": "5b8a4c46-8a72-496e-9c7d-7b49a533a736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH number: 1\n",
            "The loss at the end of this epoch is  7.040234625339508\n",
            "The loss at the end of this epoch is  6.954651653766632\n",
            "EPOCH number: 2\n",
            "The loss at the end of this epoch is  6.8429993987083435\n",
            "The loss at the end of this epoch is  6.663528472185135\n",
            "EPOCH number: 3\n",
            "The loss at the end of this epoch is  6.490560740232468\n",
            "The loss at the end of this epoch is  6.2429671585559845\n",
            "EPOCH number: 4\n",
            "The loss at the end of this epoch is  6.070838272571564\n",
            "The loss at the end of this epoch is  5.825994580984116\n",
            "EPOCH number: 5\n",
            "The loss at the end of this epoch is  5.7057976722717285\n",
            "The loss at the end of this epoch is  5.502445876598358\n"
          ]
        }
      ]
    }
  ]
}