{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2l9uu2GoRt7",
        "outputId": "3293fcaa-fcfd-44ef-f89b-2da3f0e26810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "uHoYqPtHq6Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take the dataset\n",
        "#mount the dataloader\n",
        "  #clip the sentences inside the dataloader\n",
        "  #add eos and sos and prepare the inp and gt\n",
        "\n",
        "#make the model using gru\n",
        "#write the train loop and feed the inp and loss with the target\n",
        "#add extra if for the first batch to display\n",
        "#make a custom dataset and use the dataloader"
      ],
      "metadata": {
        "id": "FiV1XEkwqK97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import numpy"
      ],
      "metadata": {
        "id": "tBeahjv9rEHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.IMDB(split='train')\n",
        "tokenizer=get_tokenizer(\"basic_english\",language='en')\n",
        "iter_for_vocab=[]\n",
        "for label,review in train_dataset:\n",
        "  iter_for_vocab.append(tokenizer(review))\n",
        "\n",
        "train_dataset=datasets.IMDB(split='train')\n",
        "\n",
        "\n",
        "vocab=build_vocab_from_iterator(\n",
        "    iter_for_vocab,\n",
        "    specials=[\"<unk>\",\"<sos>\",\"<eos>\",\"<pad>\"],\n",
        "    special_first=True,\n",
        "    min_freq=1\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "print(\"The length of the vocab is \",vocab.__len__())\n",
        "\n",
        "def text_pipeline(text):\n",
        "  return vocab.lookup_indices(tokenizer(text))\n",
        "\n",
        "# print(\"The number of labels:\",len(iter_for_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pST8nV7KrBJz",
        "outputId": "cde6d154-c0a7-40a1-ee2f-76305067f6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the vocab is  68814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_pipeline(\"Hello this is custom\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X47uDPIks3Rk",
        "outputId": "a2869db8-9f90-4100-823c-a9a281be02f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3809, 16, 12, 14161]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  input=[]\n",
        "\n",
        "  for label,review in batch:\n",
        "    tokens= text_pipeline(review)\n",
        "\n",
        "    to_put=[]\n",
        "    if (len(tokens)>10):\n",
        "      tokens=tokens[:10]\n",
        "\n",
        "    tokens.insert(0,vocab[\"<sos>\"])\n",
        "    tokens.append(vocab[\"<eos>\"])\n",
        "\n",
        "    tokens=torch.tensor(tokens,dtype=torch.long)\n",
        "\n",
        "    input.append(tokens)\n",
        "\n",
        "  input=pad_sequence(input,padding_value=vocab[\"<pad>\"],batch_first=True)\n",
        "\n",
        "  return input\n"
      ],
      "metadata": {
        "id": "5HlNTEUWtASk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader=DataLoader(train_dataset,batch_size=2,shuffle=True,collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "PYsXzvE2tcdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self,input_size,embed_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.emb=nn.Embedding(input_size,embed_size)\n",
        "    self.gru=nn.GRU(embed_size,hidden_size,batch_first=True)\n",
        "    self.lin=nn.Linear(hidden_size,input_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.emb(x)\n",
        "    outputs,hidden=self.gru(x)\n",
        "\n",
        "    outputs=self.lin(outputs)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "fh8W9G2Nu-DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=MyModel(vocab.__len__(),300,512)"
      ],
      "metadata": {
        "id": "3gLRA7NlwR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function=nn.CrossEntropyLoss()\n",
        "optimiser=optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "y9K010uhwX8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "\n",
        "  model.train()\n",
        "  train_dataset=datasets.IMDB(split='train')\n",
        "  train_dataloader=DataLoader(train_dataset,batch_size=2,shuffle=True,collate_fn=collate_fn)\n",
        "\n",
        "  total_loss=0\n",
        "\n",
        "  for i,data in enumerate(train_dataloader): #2x12\n",
        "\n",
        "    input=data[:,:-1]\n",
        "    gt=data[:,1:]\n",
        "\n",
        "    if (i==0):\n",
        "      print(\"Input pair 1 to model : \",input[0])\n",
        "      print(\"Input pair 1 : \",vocab.lookup_tokens(input[0].numpy()))\n",
        "      print(\"Output pair 1 to model : \",gt[0])\n",
        "      print(\"Target pair 1 : \",vocab.lookup_tokens(gt[0].numpy()))\n",
        "\n",
        "      print(\"Input pair 1 to model : \",input[1])\n",
        "      print(\"Input pair 1 : \",vocab.lookup_tokens(input[1].numpy()))\n",
        "      print(\"Output pair 1 to model : \",gt[1])\n",
        "      print(\"Target pair 1 : \",vocab.lookup_tokens(gt[1].numpy()))\n",
        "\n",
        "    output=model(input)\n",
        "\n",
        "    # print(output.shape)\n",
        "    # print(gt.shape)\n",
        "\n",
        "    loss=loss_function(output.reshape(output.shape[0]*output.shape[1],-1),gt.reshape(-1))\n",
        "\n",
        "    total_loss+=loss.item()\n",
        "\n",
        "    print(\"Loss on batch \",i+1,\" : \",total_loss/(i+1))\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n"
      ],
      "metadata": {
        "id": "G_XHjKV_wOws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_one_epoch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7j3KpfKIyAJz",
        "outputId": "a37f00b0-c7a3-42d7-9939-30698c2cafa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input pair 1 to model :  tensor([   1,  241,   16,   29,  280,    4,   14,    9,  134, 1452,   14])\n",
            "Input pair 1 :  ['<sos>', 'saw', 'this', 'on', 'tv', '.', 'i', \"'\", 'm', 'glad', 'i']\n",
            "Output pair 1 to model :  tensor([ 241,   16,   29,  280,    4,   14,    9,  134, 1452,   14,    2])\n",
            "Target pair 1 :  ['saw', 'this', 'on', 'tv', '.', 'i', \"'\", 'm', 'glad', 'i', '<eos>']\n",
            "Input pair 1 to model :  tensor([    1,   503,   113,  1636,     6,  1883,     9, 14580,     9,   212,\n",
            "            8])\n",
            "Input pair 1 :  ['<sos>', 'despite', 'its', 'stereotypes', ',', 'virtually', \"'\", 'no-name', \"'\", 'cast', 'and']\n",
            "Output pair 1 to model :  tensor([  503,   113,  1636,     6,  1883,     9, 14580,     9,   212,     8,\n",
            "            2])\n",
            "Target pair 1 :  ['despite', 'its', 'stereotypes', ',', 'virtually', \"'\", 'no-name', \"'\", 'cast', 'and', '<eos>']\n",
            "Loss on batch  1  :  8.355917930603027\n",
            "Loss on batch  2  :  8.570513248443604\n",
            "Loss on batch  3  :  8.768712997436523\n",
            "Loss on batch  4  :  8.958849430084229\n",
            "Loss on batch  5  :  9.283462142944336\n",
            "Loss on batch  6  :  9.409993966420492\n",
            "Loss on batch  7  :  9.163786479404994\n",
            "Loss on batch  8  :  9.055083513259888\n",
            "Loss on batch  9  :  9.160591231452095\n",
            "Loss on batch  10  :  9.113038539886475\n",
            "Loss on batch  11  :  8.888166340914639\n",
            "Loss on batch  12  :  8.674182375272116\n",
            "Loss on batch  13  :  8.567554510556734\n",
            "Loss on batch  14  :  8.569239241736275\n",
            "Loss on batch  15  :  8.400289789835613\n",
            "Loss on batch  16  :  8.374277293682098\n",
            "Loss on batch  17  :  8.338718554552864\n",
            "Loss on batch  18  :  8.377474440468681\n",
            "Loss on batch  19  :  8.32731640966315\n",
            "Loss on batch  20  :  8.357229304313659\n",
            "Loss on batch  21  :  8.231475353240967\n",
            "Loss on batch  22  :  8.19839551232078\n",
            "Loss on batch  23  :  8.193538209666377\n",
            "Loss on batch  24  :  8.197764158248901\n",
            "Loss on batch  25  :  8.17837438583374\n",
            "Loss on batch  26  :  8.208804882489718\n",
            "Loss on batch  27  :  8.217311135044804\n",
            "Loss on batch  28  :  8.27123030594417\n",
            "Loss on batch  29  :  8.261977902774152\n",
            "Loss on batch  30  :  8.20103661219279\n",
            "Loss on batch  31  :  8.191774691304852\n",
            "Loss on batch  32  :  8.203513517975807\n",
            "Loss on batch  33  :  8.264773094292844\n",
            "Loss on batch  34  :  8.249371163985309\n",
            "Loss on batch  35  :  8.208413233075824\n",
            "Loss on batch  36  :  8.163243081834581\n",
            "Loss on batch  37  :  8.139708055032266\n",
            "Loss on batch  38  :  8.118526684610467\n",
            "Loss on batch  39  :  8.113063726669703\n",
            "Loss on batch  40  :  8.088701140880584\n",
            "Loss on batch  41  :  8.075174913173768\n",
            "Loss on batch  42  :  8.068855001812889\n",
            "Loss on batch  43  :  8.033095670300861\n",
            "Loss on batch  44  :  8.015237623995\n",
            "Loss on batch  45  :  7.993175114525689\n",
            "Loss on batch  46  :  8.012651930684628\n",
            "Loss on batch  47  :  7.953373980014883\n",
            "Loss on batch  48  :  7.962999651829402\n",
            "Loss on batch  49  :  7.947326251438686\n",
            "Loss on batch  50  :  7.953078880310058\n",
            "Loss on batch  51  :  7.958133809706744\n",
            "Loss on batch  52  :  7.931145218702463\n",
            "Loss on batch  53  :  7.938374780259043\n",
            "Loss on batch  54  :  7.972960569240429\n",
            "Loss on batch  55  :  7.973766474290327\n",
            "Loss on batch  56  :  7.9952656626701355\n",
            "Loss on batch  57  :  8.003387710504365\n",
            "Loss on batch  58  :  8.014170687774133\n",
            "Loss on batch  59  :  8.039120633723373\n",
            "Loss on batch  60  :  8.014598512649536\n",
            "Loss on batch  61  :  8.017802472974433\n",
            "Loss on batch  62  :  8.013529769835934\n",
            "Loss on batch  63  :  8.036095475393628\n",
            "Loss on batch  64  :  8.020044147968292\n",
            "Loss on batch  65  :  8.026706167367788\n",
            "Loss on batch  66  :  8.004639481053207\n",
            "Loss on batch  67  :  7.995686160984324\n",
            "Loss on batch  68  :  7.9960996683906105\n",
            "Loss on batch  69  :  7.993674008742623\n",
            "Loss on batch  70  :  7.988388667787825\n",
            "Loss on batch  71  :  7.982552884330212\n",
            "Loss on batch  72  :  7.975154360135396\n",
            "Loss on batch  73  :  7.9671509821121\n",
            "Loss on batch  74  :  7.964989166002016\n",
            "Loss on batch  75  :  7.957300542195638\n",
            "Loss on batch  76  :  7.96724722259923\n",
            "Loss on batch  77  :  7.963718494811616\n",
            "Loss on batch  78  :  7.9757385315039215\n",
            "Loss on batch  79  :  7.987960519669931\n",
            "Loss on batch  80  :  7.976287639141082\n",
            "Loss on batch  81  :  7.956783700872351\n",
            "Loss on batch  82  :  7.970249902911302\n",
            "Loss on batch  83  :  7.968671166753194\n",
            "Loss on batch  84  :  7.9359031064169745\n",
            "Loss on batch  85  :  7.941731587578269\n",
            "Loss on batch  86  :  7.915589936943942\n",
            "Loss on batch  87  :  7.8913906031641465\n",
            "Loss on batch  88  :  7.8874436671083625\n",
            "Loss on batch  89  :  7.894810071152247\n",
            "Loss on batch  90  :  7.876529926723904\n",
            "Loss on batch  91  :  7.891639185475779\n",
            "Loss on batch  92  :  7.875034524046856\n",
            "Loss on batch  93  :  7.873272413848548\n",
            "Loss on batch  94  :  7.876755186851988\n",
            "Loss on batch  95  :  7.872948194804945\n",
            "Loss on batch  96  :  7.8746952911218004\n",
            "Loss on batch  97  :  7.868622017889908\n",
            "Loss on batch  98  :  7.86453577936912\n",
            "Loss on batch  99  :  7.8689756586094095\n",
            "Loss on batch  100  :  7.8479021692276\n",
            "Loss on batch  101  :  7.841465586482888\n",
            "Loss on batch  102  :  7.848438501358032\n",
            "Loss on batch  103  :  7.839032747213123\n",
            "Loss on batch  104  :  7.821862335388477\n",
            "Loss on batch  105  :  7.820134539831252\n",
            "Loss on batch  106  :  7.818184042876622\n",
            "Loss on batch  107  :  7.807606875339401\n",
            "Loss on batch  108  :  7.802805865252459\n",
            "Loss on batch  109  :  7.809134859557545\n",
            "Loss on batch  110  :  7.813576871698553\n",
            "Loss on batch  111  :  7.815403448568808\n",
            "Loss on batch  112  :  7.790205627679825\n",
            "Loss on batch  113  :  7.790485702784715\n",
            "Loss on batch  114  :  7.7898591986873695\n",
            "Loss on batch  115  :  7.789375728109609\n",
            "Loss on batch  116  :  7.793290623303117\n",
            "Loss on batch  117  :  7.780085355807573\n",
            "Loss on batch  118  :  7.790399013939551\n",
            "Loss on batch  119  :  7.801736979925332\n",
            "Loss on batch  120  :  7.791837775707245\n",
            "Loss on batch  121  :  7.788103840567849\n",
            "Loss on batch  122  :  7.78108657774378\n",
            "Loss on batch  123  :  7.785496324058471\n",
            "Loss on batch  124  :  7.783302710902307\n",
            "Loss on batch  125  :  7.78961349105835\n",
            "Loss on batch  126  :  7.787075125981891\n",
            "Loss on batch  127  :  7.7758977657228\n",
            "Loss on batch  128  :  7.775785576552153\n",
            "Loss on batch  129  :  7.772376951321151\n",
            "Loss on batch  130  :  7.780069398880005\n",
            "Loss on batch  131  :  7.770119055536867\n",
            "Loss on batch  132  :  7.753580823089138\n",
            "Loss on batch  133  :  7.754037136422064\n",
            "Loss on batch  134  :  7.723647240382522\n",
            "Loss on batch  135  :  7.721545874630963\n",
            "Loss on batch  136  :  7.708563802873387\n",
            "Loss on batch  137  :  7.708373819824553\n",
            "Loss on batch  138  :  7.715536565020464\n",
            "Loss on batch  139  :  7.708599788679494\n",
            "Loss on batch  140  :  7.711709020818983\n",
            "Loss on batch  141  :  7.697609869300896\n",
            "Loss on batch  142  :  7.688213716090565\n",
            "Loss on batch  143  :  7.682230200800863\n",
            "Loss on batch  144  :  7.692291238241726\n",
            "Loss on batch  145  :  7.707673893303706\n",
            "Loss on batch  146  :  7.69884627322628\n",
            "Loss on batch  147  :  7.69909794963136\n",
            "Loss on batch  148  :  7.707007657837224\n",
            "Loss on batch  149  :  7.707930064041342\n",
            "Loss on batch  150  :  7.706703559557597\n",
            "Loss on batch  151  :  7.715137101167085\n",
            "Loss on batch  152  :  7.7241012532460065\n",
            "Loss on batch  153  :  7.726550792556962\n",
            "Loss on batch  154  :  7.741428924845411\n",
            "Loss on batch  155  :  7.750591141177762\n",
            "Loss on batch  156  :  7.756002253446823\n",
            "Loss on batch  157  :  7.761605874747987\n",
            "Loss on batch  158  :  7.756139343297934\n",
            "Loss on batch  159  :  7.757708967856641\n",
            "Loss on batch  160  :  7.743065406382084\n",
            "Loss on batch  161  :  7.746394514297107\n",
            "Loss on batch  162  :  7.748418397373623\n",
            "Loss on batch  163  :  7.749674862879186\n",
            "Loss on batch  164  :  7.743288501006801\n",
            "Loss on batch  165  :  7.73278149546999\n",
            "Loss on batch  166  :  7.72837354619819\n",
            "Loss on batch  167  :  7.728675738066256\n",
            "Loss on batch  168  :  7.7107689792201635\n",
            "Loss on batch  169  :  7.728224625954261\n",
            "Loss on batch  170  :  7.7192490507574645\n",
            "Loss on batch  171  :  7.710664909485488\n",
            "Loss on batch  172  :  7.697881544745246\n",
            "Loss on batch  173  :  7.693871554611735\n",
            "Loss on batch  174  :  7.701370794197609\n",
            "Loss on batch  175  :  7.705357830865043\n",
            "Loss on batch  176  :  7.702947024594653\n",
            "Loss on batch  177  :  7.705207128309261\n",
            "Loss on batch  178  :  7.715028198917261\n",
            "Loss on batch  179  :  7.7222935913661335\n",
            "Loss on batch  180  :  7.7262479874822825\n",
            "Loss on batch  181  :  7.719381876413335\n",
            "Loss on batch  182  :  7.71426968391125\n",
            "Loss on batch  183  :  7.707868731087022\n",
            "Loss on batch  184  :  7.710667474114376\n",
            "Loss on batch  185  :  7.717985842678998\n",
            "Loss on batch  186  :  7.709443962702188\n",
            "Loss on batch  187  :  7.707220027790988\n",
            "Loss on batch  188  :  7.697563763628614\n",
            "Loss on batch  189  :  7.694950339655397\n",
            "Loss on batch  190  :  7.681646345791064\n",
            "Loss on batch  191  :  7.68558376746652\n",
            "Loss on batch  192  :  7.691999041785796\n",
            "Loss on batch  193  :  7.686243058486306\n",
            "Loss on batch  194  :  7.690997552625912\n",
            "Loss on batch  195  :  7.681693704311664\n",
            "Loss on batch  196  :  7.668614719595228\n",
            "Loss on batch  197  :  7.6613448803800015\n",
            "Loss on batch  198  :  7.66449741763298\n",
            "Loss on batch  199  :  7.659973116975334\n",
            "Loss on batch  200  :  7.66263885140419\n",
            "Loss on batch  201  :  7.656551658810668\n",
            "Loss on batch  202  :  7.667367967048494\n",
            "Loss on batch  203  :  7.666976394324467\n",
            "Loss on batch  204  :  7.667297988545661\n",
            "Loss on batch  205  :  7.668937472599309\n",
            "Loss on batch  206  :  7.674181298144813\n",
            "Loss on batch  207  :  7.681318857819562\n",
            "Loss on batch  208  :  7.684981404588773\n",
            "Loss on batch  209  :  7.681644954179463\n",
            "Loss on batch  210  :  7.679655950410026\n",
            "Loss on batch  211  :  7.679773623344458\n",
            "Loss on batch  212  :  7.686917510797393\n",
            "Loss on batch  213  :  7.685378942131437\n",
            "Loss on batch  214  :  7.689553749895541\n",
            "Loss on batch  215  :  7.68960833993069\n",
            "Loss on batch  216  :  7.68892397152053\n",
            "Loss on batch  217  :  7.684502325849049\n",
            "Loss on batch  218  :  7.686112286847666\n",
            "Loss on batch  219  :  7.677891044311872\n",
            "Loss on batch  220  :  7.679216388138857\n",
            "Loss on batch  221  :  7.669844227139227\n",
            "Loss on batch  222  :  7.666307443971032\n",
            "Loss on batch  223  :  7.672727720619852\n",
            "Loss on batch  224  :  7.679001785814762\n",
            "Loss on batch  225  :  7.685831912358602\n",
            "Loss on batch  226  :  7.688764609066786\n",
            "Loss on batch  227  :  7.6804060673398595\n",
            "Loss on batch  228  :  7.682267566521962\n",
            "Loss on batch  229  :  7.684399285170709\n",
            "Loss on batch  230  :  7.697388048793958\n",
            "Loss on batch  231  :  7.693462814603533\n",
            "Loss on batch  232  :  7.696200918534706\n",
            "Loss on batch  233  :  7.691291797826218\n",
            "Loss on batch  234  :  7.691155844264561\n",
            "Loss on batch  235  :  7.688576005367523\n",
            "Loss on batch  236  :  7.684191664396706\n",
            "Loss on batch  237  :  7.678343091835956\n",
            "Loss on batch  238  :  7.682123066998329\n",
            "Loss on batch  239  :  7.687411299310469\n",
            "Loss on batch  240  :  7.687507819135984\n",
            "Loss on batch  241  :  7.681492480004971\n",
            "Loss on batch  242  :  7.6845521995843935\n",
            "Loss on batch  243  :  7.685336492679737\n",
            "Loss on batch  244  :  7.682679073732407\n",
            "Loss on batch  245  :  7.6851771675810525\n",
            "Loss on batch  246  :  7.690639680963222\n",
            "Loss on batch  247  :  7.6924176882153095\n",
            "Loss on batch  248  :  7.6941068085931965\n",
            "Loss on batch  249  :  7.694521797708719\n",
            "Loss on batch  250  :  7.686453541755676\n",
            "Loss on batch  251  :  7.690400952836907\n",
            "Loss on batch  252  :  7.686172119208744\n",
            "Loss on batch  253  :  7.690794497139369\n",
            "Loss on batch  254  :  7.686818031814155\n",
            "Loss on batch  255  :  7.684368540258968\n",
            "Loss on batch  256  :  7.685723173432052\n",
            "Loss on batch  257  :  7.681539720134513\n",
            "Loss on batch  258  :  7.68279649213303\n",
            "Loss on batch  259  :  7.6845251133082915\n",
            "Loss on batch  260  :  7.685908786150125\n",
            "Loss on batch  261  :  7.687985485084212\n",
            "Loss on batch  262  :  7.679052257355843\n",
            "Loss on batch  263  :  7.679140160745541\n",
            "Loss on batch  264  :  7.6798469072038476\n",
            "Loss on batch  265  :  7.680895239452146\n",
            "Loss on batch  266  :  7.669971058243199\n",
            "Loss on batch  267  :  7.671869350283333\n",
            "Loss on batch  268  :  7.664875410385986\n",
            "Loss on batch  269  :  7.655347394234186\n",
            "Loss on batch  270  :  7.653614737369396\n",
            "Loss on batch  271  :  7.654250168712377\n",
            "Loss on batch  272  :  7.66312129269628\n",
            "Loss on batch  273  :  7.657094554586725\n",
            "Loss on batch  274  :  7.661271252771364\n",
            "Loss on batch  275  :  7.670408391952515\n",
            "Loss on batch  276  :  7.672077117622763\n",
            "Loss on batch  277  :  7.671329463001623\n",
            "Loss on batch  278  :  7.6688330164916225\n",
            "Loss on batch  279  :  7.670326842202081\n",
            "Loss on batch  280  :  7.673294575725283\n",
            "Loss on batch  281  :  7.679919355704691\n",
            "Loss on batch  282  :  7.683468403545677\n",
            "Loss on batch  283  :  7.686700380311838\n",
            "Loss on batch  284  :  7.692346086804296\n",
            "Loss on batch  285  :  7.693843250107347\n",
            "Loss on batch  286  :  7.689132076043349\n",
            "Loss on batch  287  :  7.682986358316933\n",
            "Loss on batch  288  :  7.674604574011432\n",
            "Loss on batch  289  :  7.680877760619853\n",
            "Loss on batch  290  :  7.6770409164757565\n",
            "Loss on batch  291  :  7.679090690776655\n",
            "Loss on batch  292  :  7.678032959160739\n",
            "Loss on batch  293  :  7.674719414206495\n",
            "Loss on batch  294  :  7.66944512623508\n",
            "Loss on batch  295  :  7.669597034939265\n",
            "Loss on batch  296  :  7.667272450956139\n",
            "Loss on batch  297  :  7.664986612820866\n",
            "Loss on batch  298  :  7.6561074904947475\n",
            "Loss on batch  299  :  7.653664173489829\n",
            "Loss on batch  300  :  7.643592042128245\n",
            "Loss on batch  301  :  7.644346443917664\n",
            "Loss on batch  302  :  7.644336964910393\n",
            "Loss on batch  303  :  7.645827440538816\n",
            "Loss on batch  304  :  7.643068403789871\n",
            "Loss on batch  305  :  7.6410063141682105\n",
            "Loss on batch  306  :  7.641889030637305\n",
            "Loss on batch  307  :  7.637411800191923\n",
            "Loss on batch  308  :  7.634493515863047\n",
            "Loss on batch  309  :  7.631103642935892\n",
            "Loss on batch  310  :  7.626899576956226\n",
            "Loss on batch  311  :  7.629202194919157\n",
            "Loss on batch  312  :  7.63417244492433\n",
            "Loss on batch  313  :  7.628240062025028\n",
            "Loss on batch  314  :  7.619749599960959\n",
            "Loss on batch  315  :  7.616938063455007\n",
            "Loss on batch  316  :  7.610732044600233\n",
            "Loss on batch  317  :  7.615273968278421\n",
            "Loss on batch  318  :  7.617195947365191\n",
            "Loss on batch  319  :  7.61925212342911\n",
            "Loss on batch  320  :  7.6228944979608055\n",
            "Loss on batch  321  :  7.616592003920368\n",
            "Loss on batch  322  :  7.615463076911357\n",
            "Loss on batch  323  :  7.608684158177567\n",
            "Loss on batch  324  :  7.608587686662321\n",
            "Loss on batch  325  :  7.614134516349206\n",
            "Loss on batch  326  :  7.612937179079816\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-179735a6bb0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-3539308e3069>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str1=input(\"Enter a sample string:\")\n",
        "sample_dataset=[(1,str1)]\n",
        "sample_dataloader=DataLoader(sample_dataset,batch_size=1,collate_fn=collate_fn)\n",
        "\n",
        "for data in sample_dataloader:\n",
        "  print(data)\n",
        "  print(data.shape)\n",
        "  print(vocab.lookup_tokens(data[0].numpy()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHvo8imD1j0L",
        "outputId": "c1824d36-c73c-41a9-fd2b-7f09305d5aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sample string:hello again to another very boring and a very long review and this is fun\n",
            "tensor([[   1, 3809,  201,   11,  164,   76,  239,    8,    7,   76,  214,    2]])\n",
            "torch.Size([1, 12])\n",
            "['<sos>', 'hello', 'again', 'to', 'another', 'very', 'boring', 'and', 'a', 'very', 'long', '<eos>']\n"
          ]
        }
      ]
    }
  ]
}