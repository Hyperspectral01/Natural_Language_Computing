{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook is a practice of Text Preprocessing as referred from [Let us Start with Text Processing](https://www.kaggle.com/code/priyankdl/let-us-start-with-text-processing) by **Priyank Thakkar Sir**."
      ],
      "metadata": {
        "id": "2hg5cJBPfKVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can use **string.punctuation** to play with/replace the punctuations in a string(using **str.translate**)....."
      ],
      "metadata": {
        "id": "gJM-AECIbUhn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StzSsFYYWTcl",
        "outputId": "b5f0b835-770c-4daa-d84f-c6d8cb34cba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "<class 'dict'> {63: 44, 33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
            "Before translation:\n",
            "0                       Hello, World!\n",
            "1        This is an example sentence.\n",
            "2    Good morning - have a great day!\n",
            "3                     Why so serious?\n",
            "Name: lowered_text, dtype: object\n",
            "After translation:\n",
            "0                       Hello World\n",
            "1       This is an example sentence\n",
            "2    Good morning  have a great day\n",
            "3                   Why so serious,\n",
            "Name: lowered_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "punctuation = string.punctuation\n",
        "\n",
        "mapping = str.maketrans(\"?\", \",\", punctuation.replace(\"?\", \"\"))   #Three arguments,0 and 1 arguments are strings of equal lenght that\n",
        "                                                                  #have one to one character mapping in the translation table and the 3\n",
        "                                                                  #argument specifies the ones to be deleted\n",
        "\n",
        "\n",
        "print(\"Punctuations:\",punctuation)\n",
        "print(type(mapping),mapping)\n",
        "data = {'lowered_text': [\n",
        "    'Hello, World!',\n",
        "    'This is an example sentence.',\n",
        "    'Good morning - have a great day!',\n",
        "    'Why so serious?'\n",
        "]}\n",
        "\n",
        "trdf = pd.DataFrame(data)\n",
        "\n",
        "print(\"Before translation:\")\n",
        "print(trdf['lowered_text'].head(10))\n",
        "\n",
        "trdf['lowered_text'] = trdf[\"lowered_text\"].str.translate(mapping)\n",
        "\n",
        "print(\"After translation:\")\n",
        "print(trdf['lowered_text'].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using of **collections.Counter** to find the frequency of all the words like a map in c++ Also trying to filter the commonly used words as they could be insignificant.Basically Use of a map in string/text processing\n",
        "\n"
      ],
      "metadata": {
        "id": "XNbJBgWScvHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "text = \"\"\"\n",
        "Once upon a time, in a land far, far away, there lived a young girl named Ella. Ella was kind and gentle, and everyone in the village loved her. She lived with her stepmother and stepsisters who were not so kind. They treated her poorly and made her do all the chores around the house. Despite this, Ella remained positive and hopeful.\n",
        "\n",
        "One day, a royal ball was announced, and everyone in the village was excited to attend. Ella's stepmother and stepsisters went to the ball, but they did not allow Ella to go with them. Ella was heartbroken. However, her fairy godmother appeared and magically transformed her rags into a beautiful gown and glass slippers. She told Ella to return before midnight, as the magic would wear off then.\n",
        "\n",
        "At the ball, everyone was enchanted by Ella's beauty. The prince asked her to dance, and they danced the night away. As the clock struck midnight, Ella ran away, leaving behind one of her glass slippers. The prince searched the kingdom for the owner of the glass slipper, and when he found Ella, they lived happily ever after.\n",
        "\"\"\"\n",
        "\n",
        "words = text.lower().split()\n",
        "word_freq = Counter(words)\n",
        "\n",
        "top_5_words = [word for word, freq in word_freq.most_common()[0:5]]\n",
        "\n",
        "new_string=\"\"\n",
        "for word in text.split():\n",
        "  if word not in top_5_words:\n",
        "    new_string=new_string+\" \"+word\n",
        "\n",
        "print(\"Top 5 most frequent words:\", top_5_words)\n",
        "print(\"Filtered text:\", new_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiaZ55z7dDU_",
        "outputId": "6a61ec01-5492-46dd-cbaf-7ddf30309474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most frequent words: ['the', 'and', 'her', 'ella', 'a']\n",
            "Filtered text:  Once upon time, in land far, far away, there lived young girl named Ella. Ella was kind gentle, everyone in village loved her. She lived with stepmother stepsisters who were not so kind. They treated poorly made do all chores around house. Despite this, Ella remained positive hopeful. One day, royal ball was announced, everyone in village was excited to attend. Ella's stepmother stepsisters went to ball, but they did not allow Ella to go with them. Ella was heartbroken. However, fairy godmother appeared magically transformed rags into beautiful gown glass slippers. She told Ella to return before midnight, as magic would wear off then. At ball, everyone was enchanted by Ella's beauty. The prince asked to dance, they danced night away. As clock struck midnight, Ella ran away, leaving behind one of glass slippers. The prince searched kingdom for owner of glass slipper, when he found Ella, they lived happily ever after.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to do **Stemming** using Porter Stemmer.\n",
        "Stemming is the process of trying to take a word back to its root form so that words like 'run','ran' or 'running' can be considered as one and the same"
      ],
      "metadata": {
        "id": "wrgwjnereNwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Stemmer** or a **SnowBall Stemmer** can both be used but SnowBall Stemmer is better as it supports many languages"
      ],
      "metadata": {
        "id": "roLAU2oxe9A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use of a Snowball Stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import pandas as pd\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "data = {\n",
        "    'text_data': [\n",
        "        'running runners ran quickly',\n",
        "        'happily happy happier happiest',\n",
        "        'swimming swims swam swim',\n",
        "        'beautifully beautiful beauty beautify',\n",
        "        'jumps jumped jumping jump'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def apply_stemming(text):\n",
        "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "df[\"Stemmed_Text\"] = df[\"text_data\"].apply(apply_stemming)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(df[\"text_data\"].head(5))\n",
        "\n",
        "print(\"Stemmed Text:\")\n",
        "print(df[\"Stemmed_Text\"].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3qfw8XhelxE",
        "outputId": "4eb8113e-c18f-476c-b4c2-07cfe2a06ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "0              running runners ran quickly\n",
            "1           happily happy happier happiest\n",
            "2                 swimming swims swam swim\n",
            "3    beautifully beautiful beauty beautify\n",
            "4                jumps jumped jumping jump\n",
            "Name: text_data, dtype: object\n",
            "Stemmed Text:\n",
            "0              run runner ran quick\n",
            "1    happili happi happier happiest\n",
            "2               swim swim swam swim\n",
            "3     beauti beauti beauti beautifi\n",
            "4               jump jump jump jump\n",
            "Name: Stemmed_Text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see,the above stemmed data contains words that aren't included in the actual dictionary or the text corpus"
      ],
      "metadata": {
        "id": "tITJVQLxgEWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So solve this,we would be using Lemmatisation that can check for stemmed words in the text corpus if they actually exist or not"
      ],
      "metadata": {
        "id": "JYyDBOeNgNIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also we could specify it to reduce to a root word with a specific POS(Part of Speech) in the dictionary.Let us look at an example"
      ],
      "metadata": {
        "id": "qcl0ucz7gYtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('wordnet')    #Its important for the POS tags that are mentioned while lemmatisation\n",
        "# nltk.download('averaged_perceptron_tagger')   #My best guess is that its important for the nltk.pos_tag() function\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_tags = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
        "\n",
        "sample_data = {\n",
        "    'text_content': [\n",
        "        'cats are running swiftly',\n",
        "        'beautifully made beautiful things',\n",
        "        'swimming is a good exercise',\n",
        "        'the fox jumped over the fence',\n",
        "        'she sings very well'\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataframe = pd.DataFrame(sample_data)\n",
        "\n",
        "def lemmatize_with_pos(text):\n",
        "    words = text.split()\n",
        "    result = ''\n",
        "    for word in words:\n",
        "        tag = nltk.pos_tag([word])[0][1][0]\n",
        "        pos_tag = wordnet_tags.get(tag, wordnet.NOUN)\n",
        "        result += lemmatizer.lemmatize(word, pos_tag) + ' '\n",
        "    return result.strip()\n",
        "\n",
        "dataframe[\"Lemmatized_Text\"] = dataframe[\"text_content\"].apply(lemmatize_with_pos)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(dataframe[\"text_content\"].head(5))\n",
        "\n",
        "print(\"Lemmatized Text:\")\n",
        "print(dataframe[\"Lemmatized_Text\"].head(5))\n",
        "\n",
        "print(\"Missing Values in Lemmatized Text:\")\n",
        "print(dataframe[\"Lemmatized_Text\"].isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBTx8wgqgoHu",
        "outputId": "01eb74fe-f58e-414d-98ff-0e60e50ecd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "0             cats are running swiftly\n",
            "1    beautifully made beautiful things\n",
            "2          swimming is a good exercise\n",
            "3        the fox jumped over the fence\n",
            "4                  she sings very well\n",
            "Name: text_content, dtype: object\n",
            "Lemmatized Text:\n",
            "0                  cat be run swiftly\n",
            "1    beautifully make beautiful thing\n",
            "2             swim be a good exercise\n",
            "3       the fox jumped over the fence\n",
            "4                 she sings very well\n",
            "Name: Lemmatized_Text, dtype: object\n",
            "Missing Values in Lemmatized Text:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So lemmatisation assures that the reduced word is actually a real root word in the dictionary"
      ],
      "metadata": {
        "id": "SkLJPzBljWX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playing with **Emojis**"
      ],
      "metadata": {
        "id": "aW50zdP5jczt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are usually dictionaries or libraries that can map emojis to their descriptions or also just simply give you a dictionary."
      ],
      "metadata": {
        "id": "v_80sF_njfd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These mappings can then be used to manipulate emojis in our text data.For example we can replace emojis with their descriptions,or even omit certain other emojis."
      ],
      "metadata": {
        "id": "48BlPjvfjpyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic way is to use **.sub** function.It can be used to 2 ways:\n",
        "\n",
        "1)re.sub(\"emoji_character_encodings_to_be_replaced\",\"string_that_will_replace_emojis\",\"Original_string\")  #re here means Regular Expressions,it a basically a library\n",
        "\n",
        "2)emoticons_to_be_replaced(#Obviously in character encoding).sub(\"string_that_will_replace_the_emojis\",original_string)"
      ],
      "metadata": {
        "id": "unoo_ovLkEJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Emoji to description mapping\n",
        "UNICODE_EMO = {\n",
        "    \"🔥\": \"FIRE\",\n",
        "    \"😊\": \"SMILING_FACE\",\n",
        "    \"😂\": \"FACE_WITH_TEARS_OF_JOY\",\n",
        "    \"❤️\": \"RED_HEART\",\n",
        "    \"🌟\": \"STAR\",\n",
        "    \"💔\": \"BROKEN_HEART\",\n",
        "    \"👍\": \"THUMBS_UP\",\n",
        "    \"👎\": \"THUMBS_DOWN\"\n",
        "}\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'\\\\n', text)\n",
        "\n",
        "def emojis_to_text(text):\n",
        "    for emot, desc in UNICODE_EMO.items():\n",
        "        text = re.sub(\n",
        "            r'(' + re.escape(emot) + ')',\n",
        "            desc,\n",
        "            text\n",
        "        )\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text_with_emojis = \"Game is on 🔥 and the mood is 😊\"\n",
        "cleaned_text = emojis_to_text(text_with_emojis)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text_with_emojis)\n",
        "\n",
        "print(\"Text with Emojis Replaced:\")\n",
        "print(cleaned_text)\n",
        "\n",
        "print(\"Another Function:\")\n",
        "print(remove_emoji(text_with_emojis))\n",
        "\n",
        "#Basically two different methods of replacing emojis that are slightly different.\n",
        "# emojis_to_text() can replace individual emojis where the remove_emojis() replaces all and any occurence of emojis that are compiled with a common string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KlSo4mXkvg0",
        "outputId": "1a4e9afe-bfcb-4296-835e-4aea0a77742a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Game is on 🔥 and the mood is 😊\n",
            "Text with Emojis Replaced:\n",
            "Game is on FIRE and the mood is SMILING_FACE\n",
            "Another Function:\n",
            "Game is on \\n and the mood is \\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same **sub()** can be used for strings other than emojis character encodings.For example **html tags** or even to replace certain cuss words in the online world"
      ],
      "metadata": {
        "id": "PkIf98FAlaRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def strip_html_tags(content):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub('', content)\n",
        "\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head><title>Sample Page</title></head>\n",
        "<body>\n",
        "<h1>Welcome to My Page</h1>\n",
        "<p>This is a <strong>sample</strong> paragraph.</p>\n",
        "<a href=\"https://example.com\">Visit Example</a>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "clean_text = strip_html_tags(html_content)\n",
        "\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBor_IYvlpJm",
        "outputId": "cfbedf8b-8148-44de-c2b9-f879ac895486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sample Page\n",
            "\n",
            "Welcome to My Page\n",
            "This is a sample paragraph.\n",
            "Visit Example\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally there is also a **spellchecker** library that is used to correct any syntactically wrong words,i.e. map and replace it to the nearest word in terms of syntax and semantics both"
      ],
      "metadata": {
        "id": "sXiRF9hnr0_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb317PyLs_WX",
        "outputId": "20037301-5925-4d01-e45a-c84fd6bf7fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spell.correction()**:To correct the syntax of the word\n",
        "\n",
        "**spell.unknown(list_of_words)**:To get a list of all the mispelled words in the list(Basically the words not available in the dictionary)"
      ],
      "metadata": {
        "id": "9bffe7sJtZQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "text_with_typos = \"The quik brown fox jumps ovr the lazi dog.\"\n",
        "\n",
        "def correct_spelling(text):\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in spell.unknown(words):\n",
        "                corrected_words.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "corrected_text = correct_spelling(text_with_typos)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text_with_typos)\n",
        "\n",
        "print(\"Corrected Text:\")\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miTUwMLIsHyJ",
        "outputId": "b10a1fd6-1b88-45c6-ccb4-1cf0ece80e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "The quik brown fox jumps ovr the lazi dog.\n",
            "Corrected Text:\n",
            "The quit brown fox jumps or the lazy dog\n"
          ]
        }
      ]
    }
  ]
}